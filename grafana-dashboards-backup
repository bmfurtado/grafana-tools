#!/usr/bin/python -tt
#
import datetime
import json
import logging
import logging.handlers
import optparse
import os
import codecs
import re
import shutil
import sys
import tarfile
import tempfile
import urllib
import urllib2


def grafana_dashboard_backups(elastic_host, elastic_port, dest_dir, debug=False):
  '''Saves all Grafana dashboards stored in Elasticsearch in a tarball
  
     Uses Elasticsearch API to get all saved dashboards into a tarball, the 
     tarball contains one file per dashboard and is available in destination dir
     which is parameterized (defaults to /var/opt/grafana-dashboards-backups).'''
  
  #Set Logging to syslog
  try:
    logger = logging.getLogger(__name__)
    if debug:
      logger.setLevel(logging.DEBUG)
    else:
      logger.setLevel(logging.INFO)
  
    formatter = logging.Formatter('%(pathname)s: %(message)s')
  
    syslog_handler = logging.handlers.SysLogHandler(address = '/dev/log')
    syslog_handler.setFormatter(formatter)
    logger.addHandler(syslog_handler)
  except Exception:
    logging.info('Could not set syslog logging handler')


  #urllib2.install_opener(urllib2.build_opener(urllib2.HTTPHandler(debuglevel=1)))


  def search(url, params):
    search_url = '%s?%s' %  (url, urllib.urlencode(params))
    try:
      response = urllib2.urlopen(search_url, timeout=5)
    except urllib2.URLError as e:
      if debug:
        logger.critical('Failed accessing: %s %s\n' % (search_url, e.read()))
      logger.critical(e)
      sys.exit(1)

    return json.load(response)

  def scan():
    return search(dashboards_url, {'search_type': 'scan', 'scroll': scroll_time})

  def scroll(scroll_id):
    url = '%s/%s' % (scroll_url, scroll_id)
    return search(url, {'scroll': scroll_time})
  
  def cam_case_convert(name):
    '''strips spaces and replace CamCasing.cam with cam_casing_cam'''
  
    s1 = re.sub('([^._])([A-Z][a-z]+)', r'\1_\2', name.replace(' ',''))
    s1 = s1.replace('.','_')
    return re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()
  
  # Conveniance vars
  es_url         = 'http://%s:%s' % (elastic_host, elastic_port)
  dashboards_url = '%s/%s' % (es_url, 'grafana-dash/dashboard/_search')
  scroll_url     = '%s/_search/scroll' % es_url
  scroll_time    = '1m'
  work_tmp_dir   = tempfile.mkdtemp()
  utc_datetime   = datetime.datetime.utcnow()
  formatted_time = utc_datetime.strftime("%Y-%m-%d-%H%MZ")
  
  if debug:
    logger.info('Grabbing grafana dashboards from: %s \n' % dashboards_url)

  scroll_id = scan()['_scroll_id']

  # Create a tarball with all the dashboards and move to target dir
  
  tarball = os.path.join(work_tmp_dir, 'grafana_dashboards_backup_' +
      formatted_time + '.tar.gz')
  tar = tarfile.open(tarball, 'w:gz')

  if not scroll_id:
    raise Exception('Failed to get scroll id')

  while 1:
    data      = scroll(scroll_id)
    hit_count = len(data['hits']['hits'])
    scroll_id = data['_scroll_id']
    if hit_count == 0:
      break

    try:
      for hit in data['hits']['hits']:
        dashboard_definition = json.loads(hit['_source']['dashboard'])
        dashboard_file_name = os.path.join(work_tmp_dir,
      cam_case_convert(hit['_id']) + '_' + formatted_time + '.json')
        dashboard_file = codecs.open(dashboard_file_name, 'w', 'utf-8')
        json.dump(dashboard_definition, dashboard_file, sort_keys = True, indent = 2,
      ensure_ascii=False)
        logger.info('Added %s to the dashboards backup tarball' % hit['_id'])
        dashboard_file.close()
        tar.add(dashboard_file_name)
    except Exception as e:
      logging.critical(e)
      sys.exit(1)
  
  try:
    tar.close()
    tarball_file = os.path.basename(tarball)
    tarball_dest = os.path.join(dest_dir, tarball_file)
    os.rename(tarball,tarball_dest)
    logger.info('New grafana dashboards backup at %s' % tarball_dest)
  except Exception as e:
    logging.critical('Failed to move tarball to %s' % dest_dir)
    logging.critical(e)
    sys.exit(1)
  
  # Clean up
  try:
    shutil.rmtree(work_tmp_dir)
  except Exception as e:
    logging.critical(e)
    sys.exit(1)

def main():
  parser = optparse.OptionParser()
  
  parser.add_option('-D', '--debug',
        action  = 'store_true',
        default = False,
        dest    = 'debug',
        help    = 'Debug output (very noisy)')
  
  parser.add_option('-e', '--grafana-elasticsearch-host',
    dest    = 'elastic_host',
    help    = 'The elastic search host FQDN used by grafana',
    metavar = 'ELASTIC_SEARCH_HOST')
  
  parser.add_option('-p', '--grafana-elasticsearch-port',
    default = '9200',
    dest    = 'elastic_port',
    help    = 'The elastic search port used by grafana',
    metavar = 'ELASTIC_SEARCH_PORT')
  
  parser.add_option('-t', '--dest-dir',
    default = '/var/opt/grafana-dashboards-backups',
    dest    = 'dest_dir',
    help    = 'The destination directory where dashboards will be saved',
    metavar = 'DEST_DIR')
  
  (options, args) = parser.parse_args()
  
  if not options.elastic_host:
    parser.error('An elastic search host is required')
  
  elastic_host = options.elastic_host
  elastic_port = options.elastic_port
  dest_dir = options.dest_dir
  debug = options.debug

  grafana_dashboard_backups(elastic_host, elastic_port, dest_dir)

if __name__ == '__main__':
  main()
